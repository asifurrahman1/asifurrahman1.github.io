---
layout: archive
title: "Research"
permalink: /portfolio/
author_profile: true
---
<table style="width:100%;border:none;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="https://asifurrahman1.github.io/images/rendered_fig.gif" alt="platoon" width="170" height="170">
          </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
               <h3>Safe Reinforcement Learning </h3>
             <p> 
              Current safe reinforcement learning techniques tend to restrict exploration to ensure safety, which is counterproductive to the natural exploration-exploitation learning mechanism of reinforcement learning. 
              In this research, we explored a new direction for safe RL, where safety is learned as a behavior that excludes behaviors responsible for most safety violations. Our current findings indicates this approch very promising.
              We are now extending this work to multi-agent system and planning to cover POMDP and IMDPs. 
              </p>
           </td>
       </tr>	
 <tr>
 <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="https://asifurrahman1.github.io/images/LUF-RL (2).png" alt="platoon" width="170" height="170">
    </td>
     <td style="padding:20px;width:75%;vertical-align:middle">
         <h3>Pointwise Model Stability Cause and Effect</h3>
       <p> 
        Model stability has emerged as a significant concern in machine learning (ML) research, as it can be a root cause of unfairness in decision-making processes.
        A recent study, known as Leave-One-Out Unfairness (LUF), illustrates how a single data point within a dataset can substantially impact an ML model. 
         In this research, we explore methods to approximate and identify these data points that exert a pronounced influence on a model's stability.
        </p>
     </td>
 </tr>	
<tr>
<tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="https://asifurrahman1.github.io/images/XRL_result (2).png" alt="platoon" width="170" height="170">
          </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
               <h3>Safe Explainable Reinforcement Learning (Safe-XRL)</h3>
             <p> 
              This research focuses on explaining an agent's behavior through natural language dialects. 
               We are investigating importance of explainability from the prespective of safety vulnerability, formulate suitable performance metrics to represent the user's convincingness while measuring the accuracy of the XRL model with respect to the actual agent's policy. 
               The current direction of this research is leaning toward integrating a graph neural network for policy decomposition to account for scalability and robustness.
              </p>
           </td>
       </tr>	
 <tr>
          
</tbody></table>

<!-- 
{% include base_path %}


{% for post in site.portfolio %}
  {% include archive-single.html %}
{% endfor %} -->

